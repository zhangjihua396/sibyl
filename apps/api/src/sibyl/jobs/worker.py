"""arq worker - processes background jobs.

Run with: uv run arq sibyl.jobs.WorkerSettings

This worker processes:
- crawl_source: Full documentation crawling
- sync_source: Recalculate source stats
"""

import asyncio
from datetime import UTC, datetime
from typing import Any
from uuid import UUID

import structlog
from arq.connections import RedisSettings
from sqlalchemy import func, select
from sqlmodel import col

from sibyl.config import settings
from sibyl.db import (
    AgentMessage,
    CrawledDocument,
    CrawlSource,
    CrawlStatus,
    DocumentChunk,
    get_session,
)
from sibyl.db.models import utcnow_naive

log = structlog.get_logger()


async def _safe_broadcast(event: str, data: dict[str, Any], *, org_id: str | None) -> None:
    """Broadcast event via Redis pub/sub (worker runs in separate process).

    The worker process doesn't have WebSocket connections, so we must
    publish to Redis pub/sub which the API process subscribes to.
    """
    try:
        from sibyl.api.pubsub import publish_event

        await publish_event(event, data, org_id=org_id)
    except Exception:
        log.debug("Broadcast failed (Redis unavailable)", event=event)


def get_redis_settings() -> RedisSettings:
    """Get Redis connection settings."""
    return RedisSettings(
        host=settings.falkordb_host,
        port=settings.falkordb_port,
        password=settings.falkordb_password,
        database=settings.redis_jobs_db,
    )


async def startup(ctx: dict[str, Any]) -> None:
    """Worker startup - initialize resources."""
    from sibyl.banner import log_banner
    from sibyl_core.logging import configure_logging

    # Reconfigure logging for worker (overrides API default)
    configure_logging(service_name="worker")

    log_banner(component="worker")
    log.info("Job worker online")
    ctx["start_time"] = datetime.now(UTC)


async def shutdown(ctx: dict[str, Any]) -> None:  # noqa: ARG001
    """Worker shutdown - cleanup resources."""
    log.info("Job worker shutting down")


async def crawl_source(
    ctx: dict[str, Any],  # noqa: ARG001
    source_id: str,
    *,
    max_pages: int = 100,
    max_depth: int = 3,
    generate_embeddings: bool = True,
) -> dict[str, Any]:
    """Crawl a documentation source.

    This is the main crawl job that:
    1. Fetches source from DB
    2. Runs the ingestion pipeline
    3. Updates source status
    4. Returns stats

    Args:
        ctx: arq context
        source_id: UUID of source to crawl
        max_pages: Maximum pages to crawl
        max_depth: Maximum link depth
        generate_embeddings: Whether to generate embeddings

    Returns:
        Dict with crawl stats
    """
    from sibyl.crawler import IngestionPipeline

    log.info(
        "Starting crawl job",
        source_id=source_id,
        max_pages=max_pages,
        max_depth=max_depth,
    )

    # Get source and update status
    async with get_session() as session:
        source = await session.get(CrawlSource, UUID(source_id))
        if not source:
            raise ValueError(f"Source not found: {source_id}")

        # Update status to IN_PROGRESS
        source.crawl_status = CrawlStatus.IN_PROGRESS
        source.last_error = None
        await session.flush()  # Persist status change before expunge

        # Detach for background processing
        source_name = source.name
        organization_id = str(source.organization_id)
        session.expunge(source)

    # Broadcast start event
    await _safe_broadcast(
        "crawl_started",
        {
            "source_id": source_id,
            "source_name": source_name,
            "max_pages": max_pages,
        },
        org_id=organization_id,
    )

    # Progress callback: update DB + broadcast after each document
    async def on_progress(stats: Any, chunks_added: int) -> None:
        """Update source stats and broadcast progress after each document."""
        async with get_session() as session:
            db_source = await session.get(CrawlSource, UUID(source_id))
            if db_source:
                db_source.document_count = stats.documents_stored
                db_source.chunk_count = stats.chunks_created

        await _safe_broadcast(
            "crawl_progress",
            {
                "source_id": source_id,
                "source_name": source_name,
                "documents_crawled": stats.documents_crawled,
                "documents_stored": stats.documents_stored,
                "chunks_created": stats.chunks_created,
                "chunks_added": chunks_added,
                "errors": stats.errors,
            },
            org_id=organization_id,
        )

    # Run ingestion
    try:
        async with IngestionPipeline(
            organization_id, generate_embeddings=generate_embeddings
        ) as pipeline:
            stats = await pipeline.ingest_source(
                source,
                max_pages=max_pages,
                max_depth=max_depth,
                on_progress=on_progress,
            )

        # Update source with results
        async with get_session() as session:
            db_source = await session.get(CrawlSource, UUID(source_id))
            if db_source:
                db_source.crawl_status = (
                    CrawlStatus.COMPLETED if stats.errors == 0 else CrawlStatus.PARTIAL
                )
                db_source.current_job_id = None  # Clear job ID on completion
                db_source.last_crawled_at = utcnow_naive()
                db_source.document_count = stats.documents_stored
                db_source.chunk_count = stats.chunks_created

        result = {
            "source_id": source_id,
            "source_name": source_name,
            "documents_crawled": stats.documents_crawled,
            "documents_stored": stats.documents_stored,
            "chunks_created": stats.chunks_created,
            "embeddings_generated": stats.embeddings_generated,
            "errors": stats.errors,
            "duration_seconds": stats.duration_seconds,
        }

        # Broadcast completion
        await _safe_broadcast("crawl_complete", result, org_id=organization_id)

        log.info("Crawl job complete", **result)
        return result

    except Exception as e:
        # Update source with error
        async with get_session() as session:
            db_source = await session.get(CrawlSource, UUID(source_id))
            if db_source:
                db_source.crawl_status = CrawlStatus.FAILED
                db_source.current_job_id = None  # Clear job on failure
                db_source.last_error = str(e)[:1000]

        await _safe_broadcast(
            "crawl_complete",
            {"source_id": source_id, "error": str(e)},
            org_id=organization_id,
        )

        log.exception("Crawl job failed", source_id=source_id)
        raise


async def sync_source(ctx: dict[str, Any], source_id: str) -> dict[str, Any]:  # noqa: ARG001
    """Sync source stats from actual data.

    Recalculates document_count, chunk_count, and fixes status.

    Args:
        ctx: arq context
        source_id: UUID of source to sync

    Returns:
        Dict with sync results
    """
    log.info("Starting sync job", source_id=source_id)

    async with get_session() as session:
        source = await session.get(CrawlSource, UUID(source_id))
        if not source:
            raise ValueError(f"Source not found: {source_id}")
        organization_id = str(source.organization_id)

        # Count actual documents
        doc_result = await session.execute(
            select(func.count(CrawledDocument.id)).where(
                col(CrawledDocument.source_id) == UUID(source_id)
            )
        )
        doc_count = doc_result.scalar() or 0

        # Count actual chunks
        chunk_result = await session.execute(
            select(func.count(DocumentChunk.id))
            .join(CrawledDocument)
            .where(col(CrawledDocument.source_id) == UUID(source_id))
        )
        chunk_count = chunk_result.scalar() or 0

        # Update source
        old_status = source.crawl_status
        old_doc_count = source.document_count
        old_chunk_count = source.chunk_count

        source.document_count = doc_count
        source.chunk_count = chunk_count

        if doc_count > 0 and source.crawl_status == CrawlStatus.IN_PROGRESS:
            source.crawl_status = CrawlStatus.COMPLETED
            source.current_job_id = None  # Clear job on sync completion
            if source.last_crawled_at is None:
                source.last_crawled_at = utcnow_naive()
        elif doc_count == 0 and source.crawl_status == CrawlStatus.IN_PROGRESS:
            source.crawl_status = CrawlStatus.PENDING
            source.current_job_id = None  # Clear job on sync reset

        result = {
            "source_id": source_id,
            "document_count": doc_count,
            "chunk_count": chunk_count,
            "status": source.crawl_status.value,
            "changes": {
                "status": f"{old_status.value} -> {source.crawl_status.value}"
                if old_status != source.crawl_status
                else None,
                "document_count": f"{old_doc_count} -> {doc_count}"
                if old_doc_count != doc_count
                else None,
                "chunk_count": f"{old_chunk_count} -> {chunk_count}"
                if old_chunk_count != chunk_count
                else None,
            },
        }

    log.info("Sync job complete", **result)
    await _safe_broadcast("crawl_sync_complete", result, org_id=organization_id)
    return result


async def create_entity(
    ctx: dict[str, Any],  # noqa: ARG001
    entity_data: dict[str, Any],
    entity_type: str,
    group_id: str,
    relationships: list[dict[str, Any]] | None = None,
    auto_link_params: dict[str, Any] | None = None,
) -> dict[str, Any]:
    """Create entity asynchronously via Graphiti.

    This job runs in the background so callers get fast responses while
    Graphiti handles LLM-powered entity extraction and relationship discovery.

    Args:
        ctx: arq context
        entity_data: Serialized entity dict (from entity.model_dump())
        entity_type: Type string (episode, pattern, task, project)
        group_id: Organization ID
        relationships: Optional list of explicit relationships to create
        auto_link_params: Parameters for auto-link discovery (always runs if provided)

    Returns:
        Dict with creation results
    """
    from sibyl_core.graph.client import get_graph_client
    from sibyl_core.graph.entities import EntityManager
    from sibyl_core.graph.relationships import RelationshipManager
    from sibyl_core.models.entities import (
        Entity,
        Episode,
        Pattern,
        Relationship,
        RelationshipType,
    )
    from sibyl_core.models.tasks import Project, Task

    relationships = relationships or []

    log.info(
        "create_entity_started",
        entity_id=entity_data.get("id"),
        entity_type=entity_type,
        relationships_count=len(relationships),
    )

    try:
        client = await get_graph_client()
        entity_manager = EntityManager(client, group_id=group_id)

        # Reconstruct the entity from serialized data
        entity: Entity
        if entity_type == "task":
            entity = Task.model_validate(entity_data)
        elif entity_type == "project":
            entity = Project.model_validate(entity_data)
        elif entity_type == "pattern":
            entity = Pattern.model_validate(entity_data)
        else:
            entity = Episode.model_validate(entity_data)

        # Use create_direct() for structured entities (faster, generates embeddings)
        # Use create() for episodes (LLM extraction may add value)
        if entity_type in ("task", "project", "pattern"):
            created_id = await entity_manager.create_direct(entity)
        else:
            created_id = await entity_manager.create(entity)

        log.info(
            "create_entity_graph_created",
            entity_id=created_id,
            entity_type=entity_type,
        )

        # Relationship manager for explicit and auto-discovered relationships
        relationship_manager = RelationshipManager(client, group_id=group_id)

        # Create explicit relationships (BELONGS_TO, DEPENDS_ON, etc.)
        relationships_created = 0
        for rel_data in relationships:
            try:
                rel_type = RelationshipType(rel_data.get("type", "RELATED_TO"))
                rel = Relationship(
                    id=rel_data.get("id"),
                    source_id=rel_data.get("source_id"),
                    target_id=rel_data.get("target_id"),
                    relationship_type=rel_type,
                    metadata=rel_data.get("metadata", {}),
                )
                await relationship_manager.create(rel)
                relationships_created += 1
                log.debug(
                    "create_entity_relationship_created",
                    rel_type=rel_type.value,
                    source=rel_data.get("source_id"),
                    target=rel_data.get("target_id"),
                )
            except Exception as e:
                log.warning(
                    "create_entity_relationship_failed",
                    error=str(e),
                    rel_data=rel_data,
                )

        # Auto-link: discover related entities via similarity search
        auto_links_created = 0
        if auto_link_params:
            try:
                from sibyl_core.tools.core import _auto_discover_links

                auto_link_results = await _auto_discover_links(
                    entity_manager=entity_manager,
                    title=auto_link_params.get("title", ""),
                    content=auto_link_params.get("content", ""),
                    technologies=auto_link_params.get("technologies", []),
                    category=auto_link_params.get("category"),
                    exclude_id=created_id,
                    threshold=0.75,
                    limit=5,
                )

                for linked_id, score in auto_link_results:
                    try:
                        rel = Relationship(
                            id=f"rel_{created_id}_references_{linked_id}",
                            source_id=created_id,
                            target_id=linked_id,
                            relationship_type=RelationshipType.RELATED_TO,
                            metadata={
                                "auto_linked": True,
                                "similarity_score": score,
                            },
                        )
                        await relationship_manager.create(rel)
                        auto_links_created += 1
                        log.debug(
                            "create_entity_auto_link_created",
                            target=linked_id,
                            score=score,
                        )
                    except Exception as e:
                        log.warning("create_entity_auto_link_failed", error=str(e))

                log.info(
                    "create_entity_auto_link_complete",
                    entity_id=created_id,
                    links_found=len(auto_link_results),
                )
            except Exception as e:
                log.warning("create_entity_auto_link_search_failed", error=str(e))

        result = {
            "entity_id": created_id,
            "entity_type": entity_type,
            "relationships_created": relationships_created,
            "auto_links_created": auto_links_created,
        }

        # Broadcast entity creation event
        await _safe_broadcast(
            "entity_created",
            {
                "id": created_id,
                "entity_type": entity_type,
                "name": entity_data.get("name"),
            },
            org_id=group_id,
        )

        log.info("create_entity_completed", **result)
        return result

    except Exception as e:
        log.exception(
            "create_entity_failed",
            error=str(e),
            entity_id=entity_data.get("id"),
        )
        raise


async def create_learning_episode(
    ctx: dict[str, Any],  # noqa: ARG001
    task_data: dict[str, Any],
    group_id: str,
) -> dict[str, Any]:
    """Create a learning episode from a completed task.

    This job runs in the background so task completion returns fast while
    Graphiti handles LLM-powered entity extraction from the learnings.

    Args:
        ctx: arq context
        task_data: Serialized task dict (from task.model_dump())
        group_id: Organization ID

    Returns:
        Dict with episode creation results
    """
    from sibyl_core.graph.client import get_graph_client
    from sibyl_core.graph.entities import EntityManager
    from sibyl_core.graph.relationships import RelationshipManager
    from sibyl_core.models.entities import (
        EntityType,
        Episode,
        Relationship,
        RelationshipType,
    )
    from sibyl_core.models.tasks import Task

    task = Task.model_validate(task_data)

    log.info(
        "create_learning_episode_started",
        task_id=task.id,
        task_title=task.title,
    )

    try:
        client = await get_graph_client()
        entity_manager = EntityManager(client, group_id=group_id)
        relationship_manager = RelationshipManager(client, group_id=group_id)

        # Format episode content
        content_parts = [
            f"## Task: {task.title}",
            "",
            f"**Status**: {task.status}",
            f"**Feature**: {task.feature or 'N/A'}",
            f"**Technologies**: {', '.join(task.technologies)}",
        ]

        if task.actual_hours:
            content_parts.append(f"**Time Spent**: {task.actual_hours} hours")

        if task.estimated_hours and task.actual_hours:
            accuracy = (task.estimated_hours / task.actual_hours) * 100
            content_parts.append(f"**Estimation Accuracy**: {accuracy:.1f}%")

        content_parts.extend(
            [
                "",
                "### What Was Done",
                "",
                task.description,
                "",
                "### Learnings",
                "",
                task.learnings or "",
            ]
        )

        if task.blockers_encountered:
            content_parts.extend(
                [
                    "",
                    "### Blockers Encountered",
                    "",
                ]
            )
            content_parts.extend(f"- {b}" for b in task.blockers_encountered)

        if task.commit_shas:
            content_parts.extend(
                [
                    "",
                    "### Related Commits",
                    "",
                ]
            )
            content_parts.extend(f"- `{sha}`" for sha in task.commit_shas)

        # Create episode
        episode = Episode(
            id=f"episode_{task.id}",
            entity_type=EntityType.EPISODE,
            name=f"Task Completed: {task.title}",
            description=task.description,
            content="\n".join(content_parts),
            episode_type="task_completion",
            metadata={
                "task_id": task.id,
                "project_id": task.project_id,
                "feature": task.feature,
                "technologies": task.technologies,
                "complexity": task.complexity.value if task.complexity else None,
                "estimated_hours": task.estimated_hours,
                "actual_hours": task.actual_hours,
                "estimation_accuracy": (
                    task.estimated_hours / task.actual_hours
                    if task.estimated_hours and task.actual_hours
                    else None
                ),
            },
            valid_from=task.completed_at,
        )

        # Use Graphiti create for relationship discovery from learnings
        episode_id = await entity_manager.create(episode)

        log.info(
            "create_learning_episode_entity_created",
            episode_id=episode_id,
            task_id=task.id,
        )

        # Link episode back to task
        await relationship_manager.create(
            Relationship(
                id=f"rel_episode_{task.id}",
                source_id=episode_id,
                target_id=task.id,
                relationship_type=RelationshipType.DERIVED_FROM,
            )
        )

        # Inherit knowledge relationships from task
        task_relationships = await relationship_manager.get_for_entity(
            task.id,
            relationship_types=[
                RelationshipType.REQUIRES,
                RelationshipType.REFERENCES,
                RelationshipType.PART_OF,
            ],
        )

        inherited_count = 0
        for rel in task_relationships:
            try:
                await relationship_manager.create(
                    Relationship(
                        id=f"rel_episode_{episode_id}_{rel.target_id}",
                        source_id=episode_id,
                        target_id=rel.target_id,
                        relationship_type=RelationshipType.REFERENCES,
                        metadata={"inherited_from_task": task.id},
                    )
                )
                inherited_count += 1
            except Exception as e:
                log.warning(
                    "create_learning_episode_inherit_failed",
                    error=str(e),
                    target_id=rel.target_id,
                )

        result = {
            "episode_id": episode_id,
            "task_id": task.id,
            "inherited_relationships": inherited_count,
        }

        # Broadcast episode creation
        await _safe_broadcast(
            "entity_created",
            {
                "id": episode_id,
                "entity_type": "episode",
                "name": episode.name,
                "derived_from": task.id,
            },
            org_id=group_id,
        )

        log.info("create_learning_episode_completed", **result)
        return result

    except Exception as e:
        log.exception(
            "create_learning_episode_failed",
            task_id=task.id,
            error=str(e),
        )
        raise


async def update_entity(
    ctx: dict[str, Any],  # noqa: ARG001
    entity_id: str,
    updates: dict[str, Any],
    entity_type: str,
    group_id: str,
) -> dict[str, Any]:
    """Update entity fields asynchronously.

    Generic entity update job that works for any entity type.
    Runs in the background so callers get fast responses.

    Args:
        ctx: arq context
        entity_id: The entity ID to update
        updates: Dict of field names to new values
        entity_type: Type string (episode, pattern, task, project, etc.)
        group_id: Organization ID

    Returns:
        Dict with update results
    """
    from sibyl_core.graph.client import get_graph_client
    from sibyl_core.graph.entities import EntityManager

    log.info(
        "update_entity_started",
        entity_id=entity_id,
        entity_type=entity_type,
        fields=list(updates.keys()),
    )

    try:
        client = await get_graph_client()
        entity_manager = EntityManager(client, group_id=group_id)

        # Perform the update
        result = await entity_manager.update(entity_id, updates)

        if result:
            # Broadcast update event
            await _safe_broadcast(
                "entity_updated",
                {
                    "id": entity_id,
                    "entity_type": entity_type,
                    "fields": list(updates.keys()),
                },
                org_id=group_id,
            )

            log.info(
                "update_entity_completed",
                entity_id=entity_id,
                entity_type=entity_type,
                fields=list(updates.keys()),
            )

            return {
                "entity_id": entity_id,
                "entity_type": entity_type,
                "updated_fields": list(updates.keys()),
                "success": True,
            }

        log.warning("update_entity_no_changes", entity_id=entity_id)
        return {
            "entity_id": entity_id,
            "entity_type": entity_type,
            "updated_fields": [],
            "success": False,
            "message": "No changes made",
        }

    except Exception as e:
        log.exception(
            "update_entity_failed",
            entity_id=entity_id,
            entity_type=entity_type,
            error=str(e),
        )
        raise


def _get_tool_icon_and_preview(tool_name: str, tool_input: dict[str, Any]) -> tuple[str, str]:
    """Get icon name and preview text for a tool call."""
    if tool_name == "Read":
        path = tool_input.get("file_path", "file")
        # Show just filename or last 2 path segments
        short = "/".join(path.split("/")[-2:]) if "/" in path else path
        return "Page", f"`{short}`"
    if tool_name == "Write":
        path = tool_input.get("file_path", "file")
        short = "/".join(path.split("/")[-2:]) if "/" in path else path
        return "Page", f"Write `{short}`"
    if tool_name == "Edit":
        path = tool_input.get("file_path", "file")
        short = "/".join(path.split("/")[-2:]) if "/" in path else path
        return "EditPencil", f"`{short}`"
    if tool_name == "Bash":
        cmd = tool_input.get("command", "")
        # Show the command directly, truncated
        return "Code", f"`{cmd[:60]}{'...' if len(cmd) > 60 else ''}`"
    if tool_name == "Grep":
        pattern = tool_input.get("pattern", "")
        path = tool_input.get("path", "")
        path_hint = f" in {path.split('/')[-1]}" if path else ""
        return "Search", f"`{pattern[:50]}`{path_hint}"
    if tool_name == "Glob":
        return "Folder", f"`{tool_input.get('pattern', '')}`"
    if tool_name == "WebSearch":
        return "Globe", f"{tool_input.get('query', '')[:50]}"
    if tool_name == "WebFetch":
        url = tool_input.get("url", "")
        # Extract domain
        domain = url.split("//")[-1].split("/")[0] if "//" in url else url[:40]
        return "Globe", f"{domain}"
    if tool_name == "Task":
        return "User", f"{tool_input.get('description', '')[:50]}"
    if tool_name == "TodoWrite":
        return "List", "Updating todos"
    if tool_name == "LSP":
        op = tool_input.get("operation", "")
        return "Code", f"LSP {op}"
    return "Settings", tool_name


def _generate_workflow_reminder(workflow_summary: dict[str, Any]) -> str:
    """Generate a follow-up prompt reminding about Sibyl workflow.

    Args:
        workflow_summary: Workflow state from WorkflowTracker

    Returns:
        Follow-up prompt for the agent
    """
    missing_steps: list[str] = []

    if not workflow_summary.get("searched_sibyl") and not workflow_summary.get("received_context"):
        missing_steps.append("search Sibyl for relevant patterns and past learnings")

    if not workflow_summary.get("updated_task"):
        missing_steps.append("update the task status if working on a tracked task")

    if not workflow_summary.get("captured_learning"):
        missing_steps.append("capture any non-obvious learnings discovered during this work")

    if not missing_steps:
        return "Please complete the Sibyl workflow before finishing."

    steps_text = "\n- ".join(missing_steps)
    return f"""Before finishing, please complete the Sibyl workflow:

- {steps_text}

This helps preserve learnings for future sessions. Use the Sibyl MCP tools (mcp__sibyl__search, mcp__sibyl__add, mcp__sibyl__manage) to complete these steps."""


def _format_assistant_message(content: Any, timestamp: str) -> dict[str, Any]:
    """Format an AssistantMessage for UI display."""
    if isinstance(content, list):
        blocks = []
        for block in content:
            block_type = type(block).__name__
            if block_type == "TextBlock":
                blocks.append({"type": "text", "content": getattr(block, "text", "")})
            elif block_type == "ToolUseBlock":
                tool_name = getattr(block, "name", "unknown")
                tool_input = getattr(block, "input", {})
                tool_id = getattr(block, "id", "")
                if isinstance(tool_input, dict):
                    icon, preview = _get_tool_icon_and_preview(tool_name, tool_input)
                    blocks.append(
                        {
                            "type": "tool_use",
                            "tool_name": tool_name,
                            "tool_id": tool_id,
                            "icon": icon,
                            "input": tool_input,
                            "preview": preview,
                        }
                    )

        if len(blocks) == 1:
            return {"role": "assistant", "timestamp": timestamp, **blocks[0]}
        return {
            "role": "assistant",
            "type": "multi_block",
            "blocks": blocks,
            "timestamp": timestamp,
            "preview": blocks[0].get("preview", "") if blocks else "",
        }
    return {
        "role": "assistant",
        "type": "text",
        "content": str(content) if content else "",
        "timestamp": timestamp,
        "preview": str(content)[:100] if content else "",
    }


def _format_user_message(content: Any, timestamp: str) -> dict[str, Any]:
    """Format a UserMessage (usually tool results) for UI display."""
    if isinstance(content, list):
        results = []
        for block in content:
            if type(block).__name__ == "ToolResultBlock":
                tool_id = getattr(block, "tool_use_id", "")
                result_content = getattr(block, "content", "")
                is_error = getattr(block, "is_error", False)
                preview = str(result_content)[:200]
                if len(str(result_content)) > 200:
                    preview += "..."
                results.append(
                    {
                        "type": "tool_result",
                        "tool_id": tool_id,
                        "content": str(result_content),
                        "preview": preview,
                        "is_error": is_error,
                        "icon": "Xmark" if is_error else "Check",
                    }
                )
        if len(results) == 1:
            return {"role": "tool", "timestamp": timestamp, **results[0]}
        if results:
            return {
                "role": "tool",
                "type": "multi_result",
                "results": results,
                "timestamp": timestamp,
                "preview": f"{len(results)} tool results",
            }
    return {
        "role": "user",
        "type": "text",
        "content": str(content) if content else "",
        "timestamp": timestamp,
        "preview": str(content)[:100] if content else "",
    }


async def _store_agent_message(
    agent_id: str,
    org_id: str,
    message_num: int,
    formatted: dict[str, Any],
) -> None:
    """Store agent message summary to Postgres for reload persistence.

    Only stores summarized content - full tool outputs are NOT saved.
    Real-time streaming via WebSocket shows full content during execution.
    """
    role_str = formatted.get("role", "agent")
    type_str = formatted.get("type", "text")

    # Map to enum values (use lowercase strings that match Postgres enum)
    role_map = {"assistant": "agent", "tool": "system", "system": "system", "user": "user", "unknown": "system"}
    role = role_map.get(role_str, "agent")

    type_map = {
        "text": "text",
        "tool_use": "tool_call",
        "tool_result": "tool_result",
        "result": "text",
    }
    msg_type = type_map.get(type_str, "text")

    # Build content - store full content, no truncation (DB column is TEXT/unlimited)
    if type_str == "tool_use":
        content = formatted.get("preview", formatted.get("tool_name", "Tool call"))
    elif type_str == "tool_result":
        # Store full tool result content
        content = formatted.get("content", "")
    elif type_str == "multi_result":
        # Multiple results - store all content
        results = formatted.get("results", [])
        content = "\n---\n".join(r.get("content", "") for r in results)
    elif type_str == "multi_block":
        blocks = formatted.get("blocks", [])
        content = "\n".join(b.get("content", "") for b in blocks)
    else:
        content = formatted.get("content") or formatted.get("preview", "")

    # Extract tool tracking fields (stored as proper columns, not in JSONB)
    tool_id = formatted.get("tool_id")
    parent_tool_use_id = formatted.get("parent_tool_use_id")

    # Build metadata for remaining fields
    extra = {
        "icon": formatted.get("icon"),
        "tool_name": formatted.get("tool_name"),
        "is_error": formatted.get("is_error"),
    }

    # For tool calls, store full input for code viewing
    if type_str == "tool_use":
        tool_input = formatted.get("input", {})
        if tool_input:
            extra["input"] = tool_input

    # For tool results, store full content in extra as well (for UI expansion)
    if type_str == "tool_result":
        full_content = formatted.get("content", "")
        if full_content:
            extra["full_content"] = full_content

    # Remove None values
    extra = {k: v for k, v in extra.items() if v is not None}

    try:
        async with get_session() as session:
            msg = AgentMessage(
                agent_id=agent_id,
                organization_id=UUID(org_id),
                message_num=message_num,
                role=role,
                type=msg_type,
                content=content,
                tool_id=tool_id,
                parent_tool_use_id=parent_tool_use_id,
                extra=extra,
            )
            session.add(msg)
            await session.commit()
    except Exception as e:
        log.warning("Failed to store agent message", agent_id=agent_id, error=str(e))


def _format_agent_message(message: Any) -> dict[str, Any]:
    """Format a Claude SDK message for beautiful UI display."""
    msg_class = type(message).__name__
    content = getattr(message, "content", None)
    timestamp = datetime.now(UTC).isoformat()

    # Extract parent_tool_use_id for subagent message grouping
    parent_tool_use_id = getattr(message, "parent_tool_use_id", None)

    if msg_class == "AssistantMessage":
        result = _format_assistant_message(content, timestamp)
        if parent_tool_use_id:
            result["parent_tool_use_id"] = parent_tool_use_id
        return result

    if msg_class == "UserMessage":
        result = _format_user_message(content, timestamp)
        if parent_tool_use_id:
            result["parent_tool_use_id"] = parent_tool_use_id
        return result

    if msg_class == "ResultMessage":
        usage = getattr(message, "usage", None)
        cost = getattr(message, "total_cost_usd", None)
        return {
            "role": "system",
            "type": "result",
            "icon": "Dollar",
            "session_id": getattr(message, "session_id", None),
            "usage": {
                "input_tokens": getattr(usage, "input_tokens", 0) if usage else 0,
                "output_tokens": getattr(usage, "output_tokens", 0) if usage else 0,
            }
            if usage
            else None,
            "cost_usd": cost,
            "timestamp": timestamp,
            "preview": f"${cost:.4f}" if cost else "Completed",
        }

    return {
        "role": "unknown",
        "type": msg_class.lower(),
        "content": str(content) if content else "",
        "timestamp": timestamp,
        "preview": f"{msg_class}: {str(content)[:50]}" if content else msg_class,
    }


async def _generate_and_broadcast_status_hint(
    agent_id: str,
    tool_call_id: str | None,
    tool_name: str,
    tool_input: dict[str, Any] | None,
    task_id: str | None,
    agent_type: str,
    org_id: str,
) -> None:
    """Generate a Tier 3 status hint using Haiku and broadcast it.

    This runs as a background task to avoid blocking the main agent loop.
    The hint provides a clever, contextual waiting message.
    """
    if not tool_call_id:
        return

    try:
        from sibyl.agents.status import generate_status_hint

        # Get task title if we have a task_id
        task_title = None
        if task_id:
            try:
                from sibyl_core.graph.client import get_graph_client
                from sibyl_core.graph.entities import EntityManager

                client = await get_graph_client()
                manager = EntityManager(client, group_id=org_id)
                task = await manager.get(task_id)
                if task:
                    task_title = task.name
            except Exception:
                pass  # Task lookup is best-effort

        hint = generate_status_hint(tool_name, tool_input, task_title, agent_type)

        # Broadcast the hint
        await _safe_broadcast(
            "status_hint",
            {
                "agent_id": agent_id,
                "tool_call_id": tool_call_id,
                "hint": hint,
            },
            org_id=org_id,
        )

        log.debug("Broadcast status hint", agent_id=agent_id, hint=hint)

    except Exception as e:
        # Status hints are non-critical - log and continue
        log.debug("Failed to generate status hint", error=str(e))


async def run_agent_execution(  # noqa: PLR0915
    ctx: dict[str, Any],  # noqa: ARG001
    agent_id: str,
    org_id: str,
    project_id: str,
    prompt: str,
    *,
    agent_type: str = "general",
    task_id: str | None = None,
    created_by: str | None = None,
) -> dict[str, Any]:
    """Execute a Claude agent in the worker process.

    This job runs long-running AI agent tasks in the background worker,
    keeping the API responsive. Creates checkpoints only at completion.

    Args:
        ctx: arq context
        agent_id: Pre-created agent ID
        org_id: Organization ID
        project_id: Project ID
        prompt: Initial prompt for the agent
        agent_type: Type of agent
        task_id: Optional task ID
        created_by: User ID who spawned the agent

    Returns:
        Dict with execution results
    """
    from sibyl.agents import AgentRunner, WorktreeManager
    from sibyl_core.graph.client import get_graph_client
    from sibyl_core.graph.entities import EntityManager
    from sibyl_core.models import AgentCheckpoint, AgentSpawnSource, AgentStatus, AgentType

    log.info(
        "run_agent_execution_started",
        agent_id=agent_id,
        agent_type=agent_type,
        task_id=task_id,
    )

    try:
        client = await get_graph_client()
        manager = EntityManager(client, group_id=org_id)

        # Create worktree manager and agent runner
        worktree_manager = WorktreeManager(
            entity_manager=manager,
            org_id=org_id,
            project_id=project_id,
            repo_path=".",
        )

        runner = AgentRunner(
            entity_manager=manager,
            worktree_manager=worktree_manager,
            org_id=org_id,
            project_id=project_id,
        )

        # Get task if specified
        task = None
        if task_id:
            from sibyl_core.models import Task

            entity = await manager.get(task_id)
            if entity and isinstance(entity, Task):
                task = entity

        # Spawn the agent instance with pre-generated ID
        instance = await runner.spawn(
            prompt=prompt,
            agent_type=AgentType(agent_type),
            task=task,
            spawn_source=AgentSpawnSource.USER,
            create_worktree=False,
            enable_approvals=True,
            agent_id=agent_id,
        )

        # Update with created_by if provided
        if created_by:
            await manager.update(agent_id, {"created_by": created_by})

        # Broadcast that agent is now working
        await _safe_broadcast(
            "agent_status",
            {"agent_id": agent_id, "status": "working"},
            org_id=org_id,
        )

        # Track execution state (in memory only until completion)
        message_count = 0
        session_id = ""
        last_content = ""
        tool_calls: list[str] = []
        context_broadcasted = False  # Track if we've shown injected context

        # Store the initial user prompt as message #1
        message_count += 1
        initial_message = {
            "role": "user",
            "type": "text",
            "content": prompt,
            "timestamp": datetime.now(UTC).isoformat(),
            "preview": prompt[:100] + "..." if len(prompt) > 100 else prompt,
        }
        await _safe_broadcast(
            "agent_message",
            {"agent_id": agent_id, "message_num": message_count, **initial_message},
            org_id=org_id,
        )
        await _store_agent_message(agent_id, org_id, message_count, initial_message)

        # Execute agent - stream messages to UI in real-time
        log.info("run_agent_execution_starting", agent_id=agent_id)
        async for message in instance.execute():
            message_count += 1
            msg_class = type(message).__name__

            # Format message for UI
            formatted = _format_agent_message(message)

            log.debug(
                "run_agent_message",
                agent_id=agent_id,
                message_num=message_count,
                message_type=msg_class,
                content_preview=formatted.get("preview", "")[:100],
            )

            # Broadcast message to UI in real-time
            await _safe_broadcast(
                "agent_message",
                {
                    "agent_id": agent_id,
                    "message_num": message_count,
                    **formatted,
                },
                org_id=org_id,
            )

            # Store summarized message to Postgres for reload persistence
            await _store_agent_message(agent_id, org_id, message_count, formatted)

            # Broadcast injected Sibyl context (once, after first response)
            if not context_broadcasted and instance.workflow_tracker:
                injected = instance.workflow_tracker.injected_context
                if injected:
                    context_broadcasted = True
                    message_count += 1
                    context_message = {
                        "role": "system",
                        "type": "sibyl_context",
                        "content": injected,
                        "timestamp": datetime.now(UTC).isoformat(),
                        "preview": "Sibyl context injected",
                        "icon": "Sparkles",
                    }
                    await _safe_broadcast(
                        "agent_message",
                        {"agent_id": agent_id, "message_num": message_count, **context_message},
                        org_id=org_id,
                    )
                    await _store_agent_message(agent_id, org_id, message_count, context_message)

            # Track session ID
            if sid := getattr(message, "session_id", None):
                session_id = sid

            # Track tool calls for summary
            if "ToolUse" in msg_class or formatted.get("type") == "tool_use":
                tool_name = formatted.get("tool_name", "unknown")
                tool_calls.append(tool_name)

                # Generate and broadcast Tier 3 status hint (async, non-blocking)
                # Run in background so we don't block the main loop
                tool_id = formatted.get("tool_id")
                tool_input = formatted.get("input")
                asyncio.create_task(
                    _generate_and_broadcast_status_hint(
                        agent_id=agent_id,
                        tool_call_id=tool_id,
                        tool_name=tool_name,
                        tool_input=tool_input,
                        task_id=task_id,
                        agent_type=agent_type,
                        org_id=org_id,
                    )
                )

            # Keep last meaningful content for summary
            if formatted.get("content") and formatted.get("type") != "tool_result":
                last_content = formatted.get("content", "")[:500]

        # Check workflow completion and send follow-up if needed
        # Only for substantive work (5+ tool calls with code changes)
        if instance.workflow_tracker and instance.workflow_tracker.should_remind():
            workflow_summary = instance.workflow_tracker.get_workflow_summary()
            log.info("run_agent_workflow_reminder", agent_id=agent_id, **workflow_summary)

            # Send follow-up to remind about Sibyl workflow
            follow_up_prompt = _generate_workflow_reminder(workflow_summary)

            # Stream follow-up responses
            async for message in instance.send_message(follow_up_prompt):
                message_count += 1
                formatted = _format_agent_message(message)

                await _safe_broadcast(
                    "agent_message",
                    {"agent_id": agent_id, "message_num": message_count, **formatted},
                    org_id=org_id,
                )
                await _store_agent_message(agent_id, org_id, message_count, formatted)

                # Track tool calls
                if "ToolUse" in type(message).__name__ or formatted.get("type") == "tool_use":
                    tool_name = formatted.get("tool_name", "unknown")
                    tool_calls.append(tool_name)

                # Update last content
                if formatted.get("content") and formatted.get("type") != "tool_result":
                    last_content = formatted.get("content", "")[:500]

        # Create checkpoint only on completion (summary, not full history)
        from uuid import uuid4

        checkpoint_id = f"chkpt_{uuid4().hex[:12]}"
        summary = f"Completed {message_count} turns. Tools: {', '.join(tool_calls[-5:]) or 'none'}"
        checkpoint = AgentCheckpoint(
            id=checkpoint_id,
            name=f"checkpoint-{agent_id[-8:]}",
            agent_id=agent_id,
            session_id=session_id,
            conversation_history=[
                {
                    "role": "user",
                    "content": prompt,
                    "timestamp": datetime.now(UTC).isoformat(),
                    "type": "text",
                },
                {
                    "role": "system",
                    "content": summary,
                    "timestamp": datetime.now(UTC).isoformat(),
                    "type": "text",
                },
            ],
            current_step=last_content[:200] if last_content else None,
        )
        await manager.create_direct(checkpoint)

        # Update agent status to completed
        await manager.update(
            agent_id,
            {
                "status": AgentStatus.COMPLETED.value,
                "conversation_turns": message_count,
            },
        )

        result = {
            "agent_id": agent_id,
            "status": "completed",
            "turns": message_count,
            "tools_used": len(tool_calls),
        }

        # Broadcast completion via WebSocket
        await _safe_broadcast(
            "agent_status",
            {"agent_id": agent_id, "status": "completed", "turns": message_count},
            org_id=org_id,
        )

        log.info("run_agent_execution_completed", **result)
        return result

    except Exception as e:
        log.exception("run_agent_execution_failed", agent_id=agent_id, error=str(e))

        # Update agent status to failed
        try:
            client = await get_graph_client()
            manager = EntityManager(client, group_id=org_id)
            await manager.update(
                agent_id,
                {
                    "status": AgentStatus.FAILED.value,
                    "error_message": str(e),
                },
            )
        except Exception:
            log.warning("Failed to update agent status on error", agent_id=agent_id)

        # Broadcast failure via WebSocket
        await _safe_broadcast(
            "agent_status",
            {"agent_id": agent_id, "status": "failed", "error": str(e)},
            org_id=org_id,
        )

        raise


async def resume_agent_execution(
    ctx: dict[str, Any],  # noqa: ARG001
    agent_id: str,
    org_id: str,
) -> dict[str, Any]:
    """Resume an agent from its last checkpoint.

    Called when user sends a message to a terminal agent or clicks resume.
    Loads the latest checkpoint and continues the conversation.

    Args:
        ctx: arq context
        agent_id: Agent ID to resume
        org_id: Organization ID

    Returns:
        Dict with execution results
    """
    from sibyl.agents import AgentRunner, WorktreeManager
    from sibyl_core.graph.client import get_graph_client
    from sibyl_core.graph.entities import EntityManager
    from sibyl_core.models import AgentCheckpoint, AgentRecord, AgentStatus, EntityType

    log.info("resume_agent_execution_started", agent_id=agent_id)

    try:
        client = await get_graph_client()
        manager = EntityManager(client, group_id=org_id)

        # Get agent record (manager.get returns Entity, not typed model)
        agent = await manager.get(agent_id)
        if not agent or agent.entity_type != EntityType.AGENT:
            raise ValueError(f"Agent not found: {agent_id}")

        # Extract fields from metadata
        agent_meta = agent.metadata or {}
        project_id = agent_meta.get("project_id") or ""

        # Get latest checkpoint for this agent (list_by_type returns Entity, not typed models)
        checkpoints = await manager.list_by_type(entity_type=EntityType.CHECKPOINT, limit=50)

        # Debug: log checkpoint agent_ids to find match issue
        for c in checkpoints[:5]:
            chk_agent_id = (c.metadata or {}).get("agent_id")
            log.debug(
                "checkpoint_scan",
                checkpoint_id=c.id,
                checkpoint_agent_id=chk_agent_id,
                looking_for=agent_id,
                match=chk_agent_id == agent_id,
            )

        agent_checkpoints = [
            c
            for c in checkpoints
            if (c.metadata or {}).get("agent_id") == agent_id
        ]

        if not agent_checkpoints:
            raise ValueError(f"No checkpoint found for agent {agent_id}")

        latest_entity = max(
            agent_checkpoints,
            key=lambda c: c.created_at or datetime.min.replace(tzinfo=UTC),
        )

        latest_checkpoint = AgentCheckpoint.from_entity(latest_entity)

        # Extract the latest user message as the prompt
        history = latest_checkpoint.conversation_history or []
        user_messages = [m for m in history if m.get("role") == "user"]
        prompt = user_messages[-1].get("content", "Continue.") if user_messages else "Continue."

        # Create worktree manager and agent runner
        worktree_manager = WorktreeManager(
            entity_manager=manager,
            org_id=org_id,
            project_id=project_id,
            repo_path=".",
        )

        runner = AgentRunner(
            entity_manager=manager,
            worktree_manager=worktree_manager,
            org_id=org_id,
            project_id=project_id,
        )

        # Resume from checkpoint
        instance = await runner.resume_from_checkpoint(
            checkpoint=latest_checkpoint,
            prompt=prompt,
            enable_approvals=True,
        )

        # Broadcast that agent is now working
        await _safe_broadcast(
            "agent_status",
            {"agent_id": agent_id, "status": "working"},
            org_id=org_id,
        )

        # Track execution state
        message_count = 0
        session_id = latest_checkpoint.session_id or ""
        last_content = ""
        tool_calls: list[str] = []
        context_broadcasted = False

        # Execute resumed agent - stream messages to UI
        log.info("resume_agent_execution_starting", agent_id=agent_id, checkpoint=latest_checkpoint.id)
        async for message in instance.execute():
            message_count += 1
            msg_class = type(message).__name__
            formatted = _format_agent_message(message)

            log.debug(
                "resume_agent_message",
                agent_id=agent_id,
                message_num=message_count,
                message_type=msg_class,
            )

            # Broadcast to UI
            await _safe_broadcast(
                "agent_message",
                {"agent_id": agent_id, "message_num": message_count, **formatted},
                org_id=org_id,
            )
            await _store_agent_message(agent_id, org_id, message_count, formatted)

            # Broadcast Sibyl context if available
            if not context_broadcasted and instance.workflow_tracker:
                injected = instance.workflow_tracker.injected_context
                if injected:
                    context_broadcasted = True
                    message_count += 1
                    context_message = {
                        "role": "system",
                        "type": "sibyl_context",
                        "content": injected,
                        "timestamp": datetime.now(UTC).isoformat(),
                        "preview": "Sibyl context injected",
                        "icon": "Sparkles",
                    }
                    await _safe_broadcast(
                        "agent_message",
                        {"agent_id": agent_id, "message_num": message_count, **context_message},
                        org_id=org_id,
                    )
                    await _store_agent_message(agent_id, org_id, message_count, context_message)

            # Track session ID
            if sid := getattr(message, "session_id", None):
                session_id = sid

            # Track tool calls
            if "ToolUse" in msg_class or formatted.get("type") == "tool_use":
                tool_name = formatted.get("tool_name", "unknown")
                tool_calls.append(tool_name)

            # Keep last content
            if formatted.get("content") and formatted.get("type") != "tool_result":
                last_content = formatted.get("content", "")[:500]

        # Create new checkpoint
        from uuid import uuid4

        checkpoint_id = f"chkpt_{uuid4().hex[:12]}"
        summary = f"Resumed. {message_count} turns. Tools: {', '.join(tool_calls[-5:]) or 'none'}"
        checkpoint = AgentCheckpoint(
            id=checkpoint_id,
            name=f"checkpoint-{agent_id[-8:]}",
            agent_id=agent_id,
            session_id=session_id,
            conversation_history=[
                {"role": "user", "content": prompt, "timestamp": datetime.now(UTC).isoformat(), "type": "text"},
                {"role": "system", "content": summary, "timestamp": datetime.now(UTC).isoformat(), "type": "text"},
            ],
            current_step=last_content[:200] if last_content else None,
        )
        await manager.create_direct(checkpoint)

        # Update agent status
        await manager.update(
            agent_id,
            {"status": AgentStatus.COMPLETED.value, "conversation_turns": message_count},
        )

        result = {"agent_id": agent_id, "status": "completed", "turns": message_count, "resumed": True}

        await _safe_broadcast(
            "agent_status",
            {"agent_id": agent_id, "status": "completed", "turns": message_count},
            org_id=org_id,
        )

        log.info("resume_agent_execution_completed", **result)
        return result

    except Exception as e:
        log.exception("resume_agent_execution_failed", agent_id=agent_id, error=str(e))

        try:
            client = await get_graph_client()
            manager = EntityManager(client, group_id=org_id)
            await manager.update(agent_id, {"status": AgentStatus.FAILED.value, "error_message": str(e)})
        except Exception:
            log.warning("Failed to update agent status on error", agent_id=agent_id)

        await _safe_broadcast(
            "agent_status",
            {"agent_id": agent_id, "status": "failed", "error": str(e)},
            org_id=org_id,
        )

        raise


# Optional: Scheduled job to sync all sources
async def sync_all_sources(ctx: dict[str, Any]) -> dict[str, Any]:
    """Sync all sources - can be run as a cron job."""
    from sibyl.crawler.service import list_sources

    sources = await list_sources()
    synced = 0

    for source in sources:
        try:
            await sync_source(ctx, str(source.id))
            synced += 1
        except Exception as e:
            log.warning("Failed to sync source", source_id=str(source.id), error=str(e))

    return {"synced": synced, "total": len(sources)}


async def generate_status_hint(
    ctx: dict[str, Any],  # noqa: ARG001
    agent_id: str,
    tool_call_id: str,
    tool_name: str,
    tool_input: dict[str, Any] | None = None,
    task_title: str | None = None,
    agent_type: str | None = None,
    org_id: str | None = None,
) -> dict[str, Any]:
    """Generate a contextual status hint for an agent tool call.

    Uses Claude Haiku to generate clever, playful status messages
    based on the tool being used and optional task context.

    Args:
        ctx: arq context
        agent_id: Agent making the tool call
        tool_call_id: Unique ID of the tool call
        tool_name: Name of the tool (Read, Edit, Grep, etc.)
        tool_input: Tool parameters
        task_title: Optional Sibyl task for context
        agent_type: Optional agent type for context
        org_id: Organization ID for broadcast scope

    Returns:
        Dict with generated hint
    """
    from sibyl.agents.status import generate_status_hint as gen_hint

    try:
        hint = gen_hint(tool_name, tool_input, task_title, agent_type)

        # Broadcast the hint via pubsub
        await _safe_broadcast(
            "status_hint",
            {
                "agent_id": agent_id,
                "tool_call_id": tool_call_id,
                "hint": hint,
            },
            org_id=org_id,
        )

        log.debug("Generated status hint", agent_id=agent_id, hint=hint)
        return {"success": True, "hint": hint}

    except Exception as e:
        log.warning("Failed to generate status hint", error=str(e))
        return {"success": False, "error": str(e)}


# Worker configuration
class WorkerSettings:
    """arq worker settings."""

    redis_settings = get_redis_settings()

    # Job functions
    functions = [
        crawl_source,
        sync_source,
        sync_all_sources,
        create_entity,
        create_learning_episode,
        update_entity,
        run_agent_execution,
        resume_agent_execution,
        generate_status_hint,
    ]

    # Lifecycle hooks
    on_startup = startup
    on_shutdown = shutdown

    # Worker settings
    max_jobs = 3  # Max concurrent jobs
    job_timeout = 3600  # 1 hour timeout for crawl jobs
    keep_result = 86400  # Keep results for 24 hours
    poll_delay = 0.5  # Check for jobs every 0.5s


async def run_worker_async() -> None:
    """Run the arq worker in-process.

    This allows running the worker as part of the main server process
    instead of as a separate process. Useful for development and
    simpler deployments.
    """
    from arq import Worker

    settings = WorkerSettings.redis_settings
    log.info(
        "Starting in-process job worker",
        redis_host=settings.host,
        redis_port=settings.port,
        redis_db=settings.database,
        max_jobs=WorkerSettings.max_jobs,
    )

    try:
        worker = Worker(
            functions=WorkerSettings.functions,
            redis_settings=settings,
            on_startup=WorkerSettings.on_startup,
            on_shutdown=WorkerSettings.on_shutdown,
            max_jobs=WorkerSettings.max_jobs,
            job_timeout=WorkerSettings.job_timeout,
            keep_result=WorkerSettings.keep_result,
            poll_delay=WorkerSettings.poll_delay,
        )

        await worker.async_run()
    except Exception:
        log.exception("Job worker crashed")
        raise
