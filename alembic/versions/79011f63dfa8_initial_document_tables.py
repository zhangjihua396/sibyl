"""initial_document_tables

Revision ID: 79011f63dfa8
Revises:
Create Date: 2025-12-21 00:50:05.263322

"""
from collections.abc import Sequence

import pgvector.sqlalchemy.vector
import sqlalchemy as sa
import sqlmodel.sql.sqltypes
from alembic import op

# revision identifiers, used by Alembic.
revision: str = '79011f63dfa8'
down_revision: str | Sequence[str] | None = None
branch_labels: str | Sequence[str] | None = None
depends_on: str | Sequence[str] | None = None


def upgrade() -> None:
    """Upgrade schema."""
    # Enable pgvector extension for vector similarity search
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")

    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('crawl_sources',
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('name', sqlmodel.sql.sqltypes.AutoString(length=255), nullable=False),
    sa.Column('url', sqlmodel.sql.sqltypes.AutoString(length=2048), nullable=False),
    sa.Column('source_type', sa.Enum('WEBSITE', 'GITHUB', 'LOCAL', 'API_DOCS', name='sourcetype'), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('crawl_depth', sa.Integer(), nullable=False),
    sa.Column('include_patterns', sa.Text(), nullable=False),
    sa.Column('exclude_patterns', sa.Text(), nullable=False),
    sa.Column('respect_robots', sa.Boolean(), nullable=False),
    sa.Column('crawl_status', sa.Enum('PENDING', 'IN_PROGRESS', 'COMPLETED', 'FAILED', 'PARTIAL', name='crawlstatus'), nullable=False),
    sa.Column('last_crawled_at', sa.DateTime(), nullable=True),
    sa.Column('last_error', sa.Text(), nullable=True),
    sa.Column('document_count', sa.Integer(), nullable=False),
    sa.Column('chunk_count', sa.Integer(), nullable=False),
    sa.Column('total_tokens', sa.Integer(), nullable=False),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('url')
    )
    op.create_index(op.f('ix_crawl_sources_name'), 'crawl_sources', ['name'], unique=False)
    op.create_table('crawled_documents',
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('source_id', sa.Uuid(), nullable=False),
    sa.Column('url', sqlmodel.sql.sqltypes.AutoString(length=2048), nullable=False),
    sa.Column('title', sqlmodel.sql.sqltypes.AutoString(length=512), nullable=False),
    sa.Column('raw_content', sa.Text(), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('content_hash', sqlmodel.sql.sqltypes.AutoString(length=64), nullable=False),
    sa.Column('parent_url', sqlmodel.sql.sqltypes.AutoString(length=2048), nullable=True),
    sa.Column('section_path', sa.Text(), nullable=False),
    sa.Column('depth', sa.Integer(), nullable=False),
    sa.Column('language', sqlmodel.sql.sqltypes.AutoString(length=10), nullable=True),
    sa.Column('word_count', sa.Integer(), nullable=False),
    sa.Column('token_count', sa.Integer(), nullable=False),
    sa.Column('has_code', sa.Boolean(), nullable=False),
    sa.Column('is_index', sa.Boolean(), nullable=False),
    sa.Column('headings', sa.Text(), nullable=False),
    sa.Column('links', sa.Text(), nullable=False),
    sa.Column('code_languages', sa.Text(), nullable=False),
    sa.Column('crawled_at', sa.DateTime(), nullable=False),
    sa.Column('http_status', sa.Integer(), nullable=True),
    sa.ForeignKeyConstraint(['source_id'], ['crawl_sources.id'] ),
    sa.PrimaryKeyConstraint('id'),
    sa.UniqueConstraint('url')
    )
    op.create_index(op.f('ix_crawled_documents_source_id'), 'crawled_documents', ['source_id'], unique=False)
    op.create_table('document_chunks',
    sa.Column('created_at', sa.DateTime(), nullable=False),
    sa.Column('updated_at', sa.DateTime(), nullable=False),
    sa.Column('id', sa.Uuid(), nullable=False),
    sa.Column('document_id', sa.Uuid(), nullable=False),
    sa.Column('chunk_index', sa.Integer(), nullable=False),
    sa.Column('chunk_type', sa.Enum('TEXT', 'CODE', 'HEADING', 'LIST', 'TABLE', name='chunktype'), nullable=False),
    sa.Column('content', sa.Text(), nullable=False),
    sa.Column('context', sa.Text(), nullable=True),
    sa.Column('token_count', sa.Integer(), nullable=False),
    sa.Column('start_char', sa.Integer(), nullable=False),
    sa.Column('end_char', sa.Integer(), nullable=False),
    sa.Column('heading_path', sa.Text(), nullable=False),
    sa.Column('embedding', pgvector.sqlalchemy.vector.VECTOR(dim=1536), nullable=True),
    sa.Column('language', sqlmodel.sql.sqltypes.AutoString(length=50), nullable=True),
    sa.Column('is_complete', sa.Boolean(), nullable=False),
    sa.Column('has_entities', sa.Boolean(), nullable=False),
    sa.Column('entity_ids', sa.Text(), nullable=False),
    sa.ForeignKeyConstraint(['document_id'], ['crawled_documents.id'] ),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('ix_chunks_content_fts', 'document_chunks', [sa.literal_column("to_tsvector('english', content)")], unique=False, postgresql_using='gin')
    op.create_index('ix_chunks_embedding_hnsw', 'document_chunks', ['embedding'], unique=False, postgresql_using='hnsw', postgresql_with={'m': 16, 'ef_construction': 64}, postgresql_ops={'embedding': 'vector_cosine_ops'})
    op.create_index(op.f('ix_document_chunks_document_id'), 'document_chunks', ['document_id'], unique=False)
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f('ix_document_chunks_document_id'), table_name='document_chunks')
    op.drop_index('ix_chunks_embedding_hnsw', table_name='document_chunks', postgresql_using='hnsw', postgresql_with={'m': 16, 'ef_construction': 64}, postgresql_ops={'embedding': 'vector_cosine_ops'})
    op.drop_index('ix_chunks_content_fts', table_name='document_chunks', postgresql_using='gin')
    op.drop_table('document_chunks')
    op.drop_index(op.f('ix_crawled_documents_source_id'), table_name='crawled_documents')
    op.drop_table('crawled_documents')
    op.drop_index(op.f('ix_crawl_sources_name'), table_name='crawl_sources')
    op.drop_table('crawl_sources')
    # ### end Alembic commands ###
